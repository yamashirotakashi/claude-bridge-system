"""
Claude Bridge System - Task Generator
ä¼šè©±åˆ†æã¨ã‚¿ã‚¹ã‚¯ç”Ÿæˆæ©Ÿèƒ½
"""

import json
import logging
import re
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Union

from .project_context_loader import ProjectContextLoader
from .project_registry import ProjectRegistry
from .bridge_filesystem import BridgeFileSystem
from ..exceptions import TaskGenerationError, ValidationError

logger = logging.getLogger(__name__)


class TaskGenerator:
    """ã‚¿ã‚¹ã‚¯ç”Ÿæˆã‚¯ãƒ©ã‚¹
    
    Claude Desktopã§ã®ä¼šè©±å†…å®¹ã‚’åˆ†æã—ã€Claude Codeå‘ã‘ã®
    å®Ÿè¡Œå¯èƒ½ãªã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’è‡ªå‹•ç”Ÿæˆã™ã‚‹ã€‚
    """
    
    def __init__(self, 
                 context_loader: Optional[ProjectContextLoader] = None,
                 bridge_fs: Optional[BridgeFileSystem] = None):
        """
        åˆæœŸåŒ–
        
        Args:
            context_loader: ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆãƒ­ãƒ¼ãƒ€ãƒ¼
            bridge_fs: ãƒ–ãƒªãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ 
        """
        self.context_loader = context_loader if context_loader else ProjectContextLoader()
        self.bridge_fs = bridge_fs if bridge_fs else BridgeFileSystem()
        
        # ã‚¿ã‚¹ã‚¯ç”Ÿæˆãƒ«ãƒ¼ãƒ«ã®åˆæœŸåŒ–
        self._init_task_patterns()
        
        logger.info("TaskGenerator initialized")
    
    def _init_task_patterns(self) -> None:
        """ã‚¿ã‚¹ã‚¯ç”Ÿæˆãƒ‘ã‚¿ãƒ¼ãƒ³ã®åˆæœŸåŒ–"""
        self.task_patterns = {
            # å®Ÿè£…ç³»ã‚¿ã‚¹ã‚¯
            "implementation": {
                "patterns": [
                    r"(.+)ã‚’å®Ÿè£…",
                    r"(.+)æ©Ÿèƒ½ã‚’è¿½åŠ ",
                    r"(.+)ã‚’ä½œæˆ",
                    r"(.+)ã‚’é–‹ç™º",
                    r"(.+)ã‚·ã‚¹ãƒ†ãƒ ã‚’æ§‹ç¯‰"
                ],
                "priority": "high",
                "type": "implementation"
            },
            
            # ä¿®æ­£ç³»ã‚¿ã‚¹ã‚¯
            "fix": {
                "patterns": [
                    r"(.+)ã‚’ä¿®æ­£",
                    r"(.+)ãƒã‚°ã‚’ç›´",
                    r"(.+)ã‚¨ãƒ©ãƒ¼ã‚’è§£æ±º",
                    r"(.+)å•é¡Œã‚’ä¿®æ­£",
                    r"(.+)ã‚’æ”¹ä¿®"
                ],
                "priority": "high",
                "type": "bugfix"
            },
            
            # æ”¹å–„ç³»ã‚¿ã‚¹ã‚¯
            "improvement": {
                "patterns": [
                    r"(.+)ã‚’æ”¹å–„",
                    r"(.+)ã‚’æœ€é©åŒ–",
                    r"(.+)ã‚’å¼·åŒ–",
                    r"(.+)ã‚’å‘ä¸Š",
                    r"(.+)ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’"
                ],
                "priority": "medium",
                "type": "improvement"
            },
            
            # åˆ†æç³»ã‚¿ã‚¹ã‚¯
            "analysis": {
                "patterns": [
                    r"(.+)ã‚’åˆ†æ",
                    r"(.+)ã‚’èª¿æŸ»",
                    r"(.+)ã‚’æ¤œè¨¼",
                    r"(.+)ã‚’ãƒã‚§ãƒƒã‚¯",
                    r"(.+)ã‚’ç¢ºèª"
                ],
                "priority": "low",
                "type": "analysis"
            },
            
            # ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°ç³»
            "refactor": {
                "patterns": [
                    r"(.+)ã‚’ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°",
                    r"(.+)ã‚’æ•´ç†",
                    r"(.+)ã‚’å†æ§‹ç¯‰",
                    r"(.+)ã‚’çµ±ä¸€",
                    r"(.+)ã‚’å†è¨­è¨ˆ"
                ],
                "priority": "medium",
                "type": "refactor"
            }
        }
    
    def analyze_conversation(self, conversation_content: str, 
                           context_projects: Optional[List[str]] = None) -> Dict:
        """
        ä¼šè©±å†…å®¹ã‚’åˆ†æã—ã¦ã‚¿ã‚¹ã‚¯ç”Ÿæˆå¯èƒ½æ€§ã‚’è©•ä¾¡
        
        Args:
            conversation_content: ä¼šè©±ã®å†…å®¹
            context_projects: ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã«å«ã‚€ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
            
        Returns:
            åˆ†æçµæœã®è¾æ›¸
        """
        analysis = {
            "status": "success",
            "detected_projects": [],
            "task_candidates": [],
            "action_items": [],
            "complexity_score": 0,
            "confidence": 0.0,
            "recommendations": [],
            "error": None
        }
        
        try:
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡º
            detected_projects = self.context_loader.detect_project_shortcuts(conversation_content)
            analysis["detected_projects"] = detected_projects
            
            # ã‚¿ã‚¹ã‚¯å€™è£œã®æŠ½å‡º
            task_candidates = self._extract_task_candidates(conversation_content)
            analysis["task_candidates"] = task_candidates
            
            # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã®ç‰¹å®š
            action_items = self._identify_action_items(conversation_content, detected_projects)
            analysis["action_items"] = action_items
            
            # è¤‡é›‘åº¦ã‚¹ã‚³ã‚¢ã®è¨ˆç®—
            complexity_score = self._calculate_complexity(conversation_content, task_candidates)
            analysis["complexity_score"] = complexity_score
            
            # ä¿¡é ¼åº¦ã®è¨ˆç®—
            confidence = self._calculate_confidence(task_candidates, detected_projects)
            analysis["confidence"] = confidence
            
            # æ¨å¥¨äº‹é …ã®ç”Ÿæˆ
            recommendations = self._generate_recommendations(
                task_candidates, detected_projects, complexity_score
            )
            analysis["recommendations"] = recommendations
            
            logger.info(f"Conversation analysis completed: {len(task_candidates)} candidates found")
            
        except Exception as e:
            analysis["status"] = "error"
            analysis["error"] = str(e)
            logger.error(f"Conversation analysis failed: {e}")
        
        return analysis
    
    def _extract_task_candidates(self, content: str) -> List[Dict]:
        """ä¼šè©±ã‹ã‚‰ã‚¿ã‚¹ã‚¯å€™è£œã‚’æŠ½å‡º"""
        candidates = []
        
        for category, config in self.task_patterns.items():
            for pattern in config["patterns"]:
                matches = re.finditer(pattern, content, re.IGNORECASE)
                
                for match in matches:
                    candidate = {
                        "category": category,
                        "type": config["type"],
                        "priority": config["priority"],
                        "description": match.group(0),
                        "extracted_content": match.group(1) if match.groups() else match.group(0),
                        "position": match.span(),
                        "confidence": self._calculate_pattern_confidence(match.group(0))
                    }
                    candidates.append(candidate)
        
        # é‡è¤‡é™¤å»ã¨ä¿¡é ¼åº¦é †ã‚½ãƒ¼ãƒˆ
        unique_candidates = self._deduplicate_candidates(candidates)
        return sorted(unique_candidates, key=lambda x: x["confidence"], reverse=True)
    
    def _calculate_pattern_confidence(self, matched_text: str) -> float:
        """ãƒ‘ã‚¿ãƒ¼ãƒ³ãƒãƒƒãƒã®ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        base_confidence = 0.7
        
        # å…·ä½“çš„ãªã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ãŒã‚ã‚‹å ´åˆã¯ä¿¡é ¼åº¦å‘ä¸Š
        technical_keywords = [
            "API", "ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", "èªè¨¼", "ãƒ†ã‚¹ãƒˆ", "UI", "ãƒãƒƒã‚¯ã‚¨ãƒ³ãƒ‰", 
            "ãƒ•ãƒ­ãƒ³ãƒˆã‚¨ãƒ³ãƒ‰", "ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", "ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", "ãƒ­ã‚°"
        ]
        
        keyword_bonus = sum(0.05 for keyword in technical_keywords 
                           if keyword.lower() in matched_text.lower())
        
        # é•·ã•ã«ã‚ˆã‚‹èª¿æ•´
        length_factor = min(len(matched_text) / 50, 1.0) * 0.1
        
        return min(base_confidence + keyword_bonus + length_factor, 1.0)
    
    def _deduplicate_candidates(self, candidates: List[Dict]) -> List[Dict]:
        """ã‚¿ã‚¹ã‚¯å€™è£œã®é‡è¤‡ã‚’é™¤å»"""
        seen_descriptions = set()
        unique_candidates = []
        
        for candidate in candidates:
            desc_key = candidate["extracted_content"].lower().strip()
            if desc_key not in seen_descriptions:
                seen_descriptions.add(desc_key)
                unique_candidates.append(candidate)
        
        return unique_candidates
    
    def _identify_action_items(self, content: str, projects: List[str]) -> List[Dict]:
        """å…·ä½“çš„ãªã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ ã‚’ç‰¹å®š"""
        action_patterns = [
            r"(?:ã™ã‚‹|ã‚„ã‚‹|å®Ÿè¡Œ|å¯¾å¿œ)(?:å¿…è¦|ã¹ã)",
            r"(?:ä½œæˆ|å®Ÿè£…|ä¿®æ­£|æ”¹å–„)(?:ã—ã¦|ã‚’)",
            r"(?:ç¢ºèª|ãƒã‚§ãƒƒã‚¯|æ¤œè¨¼)(?:ã—ã¦|ã‚’)",
            r"(?:å¯¾å¿œ|å‡¦ç†|å®Ÿè¡Œ)(?:ã™ã¹ã|ãŒå¿…è¦)"
        ]
        
        action_items = []
        sentences = content.split('ã€‚')
        
        for sentence in sentences:
            sentence = sentence.strip()
            if not sentence:
                continue
                
            for pattern in action_patterns:
                if re.search(pattern, sentence):
                    # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆé–¢é€£æ€§ã‚’ãƒã‚§ãƒƒã‚¯
                    related_project = None
                    for project in projects:
                        if f"[{project}]" in sentence or project in sentence:
                            related_project = project
                            break
                    
                    action_item = {
                        "description": sentence,
                        "related_project": related_project,
                        "urgency": self._assess_urgency(sentence),
                        "actionable": self._assess_actionability(sentence)
                    }
                    action_items.append(action_item)
                    break
        
        return action_items
    
    def _assess_urgency(self, text: str) -> str:
        """æ–‡ã®ç·Šæ€¥åº¦ã‚’è©•ä¾¡"""
        high_urgency_keywords = ["ç·Šæ€¥", "è‡³æ€¥", "ã™ãã«", "å³åº§ã«", "é‡è¦", "ã‚¯ãƒªãƒ†ã‚£ã‚«ãƒ«"]
        medium_urgency_keywords = ["æ—©ã‚ã«", "å„ªå…ˆ", "å¿…è¦", "ã™ã¹ã"]
        
        text_lower = text.lower()
        
        if any(keyword in text_lower for keyword in high_urgency_keywords):
            return "high"
        elif any(keyword in text_lower for keyword in medium_urgency_keywords):
            return "medium"
        else:
            return "low"
    
    def _assess_actionability(self, text: str) -> float:
        """æ–‡ã®å®Ÿè¡Œå¯èƒ½æ€§ã‚’è©•ä¾¡ï¼ˆ0.0-1.0ï¼‰"""
        actionable_indicators = [
            "å®Ÿè£…", "ä½œæˆ", "ä¿®æ­£", "è¿½åŠ ", "å‰Šé™¤", "æ›´æ–°", "è¨­å®š", "æ§‹ç¯‰"
        ]
        vague_indicators = [
            "æ¤œè¨", "è€ƒãˆã‚‹", "æ€ã†", "ã‹ã‚‚ã—ã‚Œãªã„", "ã ã‚ã†", "å¯èƒ½æ€§"
        ]
        
        text_lower = text.lower()
        
        actionable_score = sum(0.2 for indicator in actionable_indicators 
                              if indicator in text_lower)
        vague_penalty = sum(0.15 for indicator in vague_indicators 
                           if indicator in text_lower)
        
        return max(0.0, min(1.0, 0.5 + actionable_score - vague_penalty))
    
    def _calculate_complexity(self, content: str, candidates: List[Dict]) -> int:
        """ã‚¿ã‚¹ã‚¯ã®è¤‡é›‘åº¦ã‚’è¨ˆç®—ï¼ˆ1-10ã‚¹ã‚±ãƒ¼ãƒ«ï¼‰"""
        base_complexity = len(candidates)
        
        # è¤‡é›‘åº¦ã‚’ä¸Šã’ã‚‹è¦å› 
        complexity_indicators = [
            ("è¤‡æ•°ã®ãƒ•ã‚¡ã‚¤ãƒ«", 2),
            ("ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹", 2),
            ("API", 1),
            ("ãƒ†ã‚¹ãƒˆ", 1),
            ("ãƒªãƒ•ã‚¡ã‚¯ã‚¿ãƒªãƒ³ã‚°", 3),
            ("çµ±åˆ", 2),
            ("ã‚»ã‚­ãƒ¥ãƒªãƒ†ã‚£", 2),
            ("ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹", 2)
        ]
        
        content_lower = content.lower()
        complexity_boost = sum(weight for keyword, weight in complexity_indicators 
                              if keyword.lower() in content_lower)
        
        return min(10, max(1, base_complexity + complexity_boost))
    
    def _calculate_confidence(self, candidates: List[Dict], projects: List[str]) -> float:
        """ã‚¿ã‚¹ã‚¯ç”Ÿæˆã®å…¨ä½“çš„ä¿¡é ¼åº¦ã‚’è¨ˆç®—"""
        if not candidates:
            return 0.0
        
        # å€™è£œã®å¹³å‡ä¿¡é ¼åº¦
        avg_candidate_confidence = sum(c["confidence"] for c in candidates) / len(candidates)
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆæ¤œå‡ºã«ã‚ˆã‚‹ä¿¡é ¼åº¦å‘ä¸Š
        project_bonus = min(len(projects) * 0.1, 0.3)
        
        # é«˜ä¿¡é ¼åº¦å€™è£œã®å­˜åœ¨ã«ã‚ˆã‚‹å‘ä¸Š
        high_conf_bonus = 0.1 if any(c["confidence"] > 0.8 for c in candidates) else 0.0
        
        return min(1.0, avg_candidate_confidence + project_bonus + high_conf_bonus)
    
    def _generate_recommendations(self, candidates: List[Dict], 
                                projects: List[str], complexity: int) -> List[str]:
        """æ¨å¥¨äº‹é …ã‚’ç”Ÿæˆ"""
        recommendations = []
        
        if not candidates:
            recommendations.append("æ˜ç¢ºãªã‚¿ã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã›ã‚“ã§ã—ãŸã€‚ã‚ˆã‚Šå…·ä½“çš„ãªæŒ‡ç¤ºã‚’å«ã‚ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
            return recommendations
        
        if not projects:
            recommendations.append("ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚·ãƒ§ãƒ¼ãƒˆã‚«ãƒƒãƒˆ([project]å½¢å¼)ã‚’ä½¿ç”¨ã™ã‚‹ã¨ã€ã‚ˆã‚Šæ­£ç¢ºãªã‚¿ã‚¹ã‚¯ç”ŸæˆãŒå¯èƒ½ã§ã™ã€‚")
        
        if complexity > 7:
            recommendations.append("è¤‡é›‘åº¦ãŒé«˜ã„ã‚¿ã‚¹ã‚¯ã§ã™ã€‚æ®µéšçš„ãªå®Ÿè£…ã‚’æ¤œè¨ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚")
        
        high_priority_count = sum(1 for c in candidates if c["priority"] == "high")
        if high_priority_count > 3:
            recommendations.append(f"{high_priority_count}å€‹ã®é«˜å„ªå…ˆåº¦ã‚¿ã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚å„ªå…ˆé †ä½ã®èª¿æ•´ã‚’æ¤œè¨ã—ã¦ãã ã•ã„ã€‚")
        
        implementation_count = sum(1 for c in candidates if c["type"] == "implementation")
        if implementation_count > 0:
            recommendations.append(f"{implementation_count}å€‹ã®å®Ÿè£…ã‚¿ã‚¹ã‚¯ãŒæ¤œå‡ºã•ã‚Œã¾ã—ãŸã€‚äº‹å‰ã®ãƒ†ã‚¹ãƒˆè¨­è¨ˆã‚’æ¨å¥¨ã—ã¾ã™ã€‚")
        
        return recommendations
    
    def generate_task_file(self, conversation_content: str, 
                          project_ids: Optional[List[str]] = None,
                          task_metadata: Optional[Dict] = None) -> Path:
        """
        ä¼šè©±å†…å®¹ã‹ã‚‰ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã‚’ç”Ÿæˆ
        
        Args:
            conversation_content: ä¼šè©±å†…å®¹
            project_ids: å¯¾è±¡ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆ
            task_metadata: è¿½åŠ ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
            
        Returns:
            ç”Ÿæˆã•ã‚ŒãŸã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
            
        Raises:
            TaskGenerationError: ã‚¿ã‚¹ã‚¯ç”Ÿæˆã«å¤±æ•—ã—ãŸå ´åˆ
        """
        try:
            # ä¼šè©±åˆ†æ
            analysis = self.analyze_conversation(conversation_content, project_ids)
            
            if analysis["status"] == "error":
                raise TaskGenerationError(f"Conversation analysis failed: {analysis['error']}")
            
            # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã®èª­ã¿è¾¼ã¿
            project_contexts = {}
            detected_projects = analysis["detected_projects"]
            
            for project_id in detected_projects:
                try:
                    context = self.context_loader.load_project_context(project_id)
                    project_contexts[project_id] = context
                except Exception as e:
                    logger.warning(f"Failed to load context for project {project_id}: {e}")
            
            # ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«å†…å®¹ã®ç”Ÿæˆ
            task_content = self._generate_task_markdown(
                conversation_content, analysis, project_contexts, task_metadata
            )
            
            # ãƒ•ã‚¡ã‚¤ãƒ«ä¿å­˜
            primary_project = detected_projects[0] if detected_projects else "general"
            task_file = self.bridge_fs.save_task_file(
                task_content, primary_project, "auto_generated"
            )
            
            logger.info(f"Task file generated: {task_file}")
            return task_file
            
        except Exception as e:
            error_msg = f"Task generation failed: {str(e)}"
            logger.error(error_msg)
            raise TaskGenerationError(error_msg, str(e))
    
    def _generate_task_markdown(self, conversation: str, analysis: Dict,
                               contexts: Dict, metadata: Optional[Dict]) -> str:
        """ã‚¿ã‚¹ã‚¯ãƒ•ã‚¡ã‚¤ãƒ«ã®Markdownå†…å®¹ã‚’ç”Ÿæˆ"""
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼æƒ…å ±
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        projects_str = ", ".join(analysis["detected_projects"]) if analysis["detected_projects"] else "N/A"
        
        content_parts = [
            "# Claude Bridge Auto-Generated Task",
            "",
            f"**Generated At**: {timestamp}",
            f"**Target Projects**: {projects_str}",
            f"**Complexity Score**: {analysis['complexity_score']}/10",
            f"**Confidence**: {analysis['confidence']:.2f}",
            "",
            "## ğŸ“‹ Original Conversation",
            "",
            "```",
            conversation.strip(),
            "```",
            "",
            "## ğŸ¯ Detected Task Candidates",
            ""
        ]
        
        # ã‚¿ã‚¹ã‚¯å€™è£œã®ä¸€è¦§
        for i, candidate in enumerate(analysis["task_candidates"], 1):
            content_parts.extend([
                f"### {i}. {candidate['description']}",
                f"- **Type**: {candidate['type']}",
                f"- **Priority**: {candidate['priority']}",
                f"- **Confidence**: {candidate['confidence']:.2f}",
                f"- **Category**: {candidate['category']}",
                ""
            ])
        
        # ã‚¢ã‚¯ã‚·ãƒ§ãƒ³ã‚¢ã‚¤ãƒ†ãƒ 
        if analysis["action_items"]:
            content_parts.extend([
                "## âš¡ Action Items",
                ""
            ])
            
            for i, item in enumerate(analysis["action_items"], 1):
                content_parts.extend([
                    f"### {i}. {item['description']}",
                    f"- **Related Project**: {item['related_project'] or 'N/A'}",
                    f"- **Urgency**: {item['urgency']}",
                    f"- **Actionability**: {item['actionable']:.2f}",
                    ""
                ])
        
        # ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆ
        if contexts:
            content_parts.extend([
                "## ğŸ“ Project Context",
                ""
            ])
            
            for project_id, context in contexts.items():
                basic_info = context.get("basic_info", {})
                content_parts.extend([
                    f"### {basic_info.get('name', project_id)}",
                    f"- **Shortcut**: {basic_info.get('shortcut', 'N/A')}",
                    f"- **Description**: {basic_info.get('description', 'N/A')}",
                    f"- **Tech Stack**: {', '.join(basic_info.get('tech_stack', []))}",
                    f"- **Path**: {basic_info.get('path', 'N/A')}",
                    ""
                ])
        
        # æ¨å¥¨äº‹é …
        if analysis["recommendations"]:
            content_parts.extend([
                "## ğŸ’¡ Recommendations",
                ""
            ])
            
            for i, rec in enumerate(analysis["recommendations"], 1):
                content_parts.append(f"{i}. {rec}")
            
            content_parts.append("")
        
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿
        if metadata:
            content_parts.extend([
                "## ğŸ“Š Metadata",
                "",
                "```json",
                json.dumps(metadata, ensure_ascii=False, indent=2),
                "```",
                ""
            ])
        
        # ãƒ•ãƒƒã‚¿ãƒ¼
        content_parts.extend([
            "---",
            "",
            "*Generated by Claude Bridge System TaskGenerator*",
            f"*Analysis ID: {datetime.now().strftime('%Y%m%d_%H%M%S')}*"
        ])
        
        return "\n".join(content_parts)
    
    def batch_generate_tasks(self, conversations: List[Dict]) -> List[Dict]:
        """
        è¤‡æ•°ã®ä¼šè©±ã‹ã‚‰ãƒãƒƒãƒã§ã‚¿ã‚¹ã‚¯ã‚’ç”Ÿæˆ
        
        Args:
            conversations: ä¼šè©±ãƒ‡ãƒ¼ã‚¿ã®ãƒªã‚¹ãƒˆ
            
        Returns:
            ç”Ÿæˆçµæœã®ãƒªã‚¹ãƒˆ
        """
        results = []
        
        for i, conv_data in enumerate(conversations):
            try:
                result = {
                    "index": i,
                    "status": "success",
                    "conversation_id": conv_data.get("id"),
                    "task_file": None,
                    "analysis": None,
                    "error": None
                }
                
                # ã‚¿ã‚¹ã‚¯ç”Ÿæˆ
                task_file = self.generate_task_file(
                    conv_data["content"],
                    conv_data.get("projects"),
                    conv_data.get("metadata")
                )
                
                result["task_file"] = str(task_file)
                result["analysis"] = self.analyze_conversation(
                    conv_data["content"], 
                    conv_data.get("projects")
                )
                
            except Exception as e:
                result = {
                    "index": i,
                    "status": "error",
                    "conversation_id": conv_data.get("id"),
                    "task_file": None,
                    "analysis": None,
                    "error": str(e)
                }
                logger.error(f"Batch task generation failed for conversation {i}: {e}")
            
            results.append(result)
        
        logger.info(f"Batch task generation completed: {len(results)} conversations processed")
        return results
    
    def get_generation_stats(self) -> Dict:
        """ã‚¿ã‚¹ã‚¯ç”Ÿæˆçµ±è¨ˆæƒ…å ±ã‚’å–å¾—"""
        try:
            # ãƒ–ãƒªãƒƒã‚¸ãƒ•ã‚¡ã‚¤ãƒ«ã‚·ã‚¹ãƒ†ãƒ ã®çµ±è¨ˆ
            fs_stats = self.bridge_fs.get_system_stats()
            
            # ã‚­ãƒ£ãƒƒã‚·ãƒ¥çµ±è¨ˆ
            cache_stats = self.context_loader.get_cache_stats()
            
            stats = {
                "filesystem": fs_stats,
                "cache": cache_stats,
                "task_patterns": len(self.task_patterns),
                "last_updated": datetime.now().isoformat()
            }
            
            return stats
            
        except Exception as e:
            logger.error(f"Failed to get generation stats: {e}")
            return {"error": str(e)}